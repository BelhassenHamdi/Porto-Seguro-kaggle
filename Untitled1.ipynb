{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamdi/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categarical_data = []\n",
    "for s in headers:\n",
    "    if s.split('_')[-1] == 'cat':\n",
    "        categarical_data.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data,columns=categarical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    573518\n",
       "1     21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 24  7 ..., 19 11 22]\n",
      "573518\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "X_iris = data[data['target']== 0]\n",
    "del X_iris['target']\n",
    "del X_iris['id']\n",
    "k_means = cluster.KMeans(n_clusters=26)\n",
    "k_means.fit(X_iris) \n",
    "\n",
    "print(k_means.labels_[::10])\n",
    "\n",
    "print(len(k_means.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus = pd.DataFrame(k_means.labels_,columns=['cluster'])\n",
    "fourat = pd.concat([data, clus], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourat['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = {}\n",
    "for i in range(26):\n",
    "    aux = \"data_\"+str(i)\n",
    "    my_data[aux]= pd.concat([fourat[fourat['cluster']==i],data[data['target']==1]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49338, 230)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['data_13'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = 230\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# correct labels\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# input data\n",
    "x = tf.placeholder(tf.float32, [None, feature_size])\n",
    "\n",
    "# build the network\n",
    "keep_prob_input = tf.placeholder(tf.float32)\n",
    "x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "W_fc1 = weight_variable([feature_size, 1200])\n",
    "b_fc1 = bias_variable([1200])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1200, 1200])\n",
    "b_fc2 = bias_variable([1200])\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "W_fc3 = weight_variable([1200, 1])\n",
    "b_fc3 = bias_variable([1])\n",
    "y = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define training step and accuracy\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# create a saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# initialize the graph\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# train\n",
    "batch_size = 100\n",
    "print(\"Startin Burn-In...\")\n",
    "saver.save(sess, 'mnist_fc_best')\n",
    "for i in range(700):\n",
    "    input_images, correct_predictions = mnist.train.next_batch(batch_size)\n",
    "    if i % (60000/batch_size) == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: input_images, y_: correct_predictions, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        # validate\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: mnist.test.images, y_: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "        print(\"Validation accuracy: %g.\" % test_accuracy)\n",
    "    sess.run(train_step, feed_dict={x: input_images, y_: correct_predictions, keep_prob_input: 0.8, keep_prob: 0.5})\n",
    "saver.restore(sess, 'mnist_fc_best')\n",
    "print(\"Starting the training...\")\n",
    "start_time = time()\n",
    "best_accuracy = 0.0\n",
    "for i in range(20*60000/batch_size):\n",
    "    input_images, correct_predictions = mnist.train.next_batch(batch_size)\n",
    "    if i % (60000/batch_size) == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: input_images, y_: correct_predictions, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        # validate\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x: mnist.test.images, y_: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "        if test_accuracy >= best_accuracy:\n",
    "            saver.save(sess, 'mnist_fc_best')\n",
    "            best_accuracy = test_accuracy\n",
    "            print(\"Validation accuracy improved: %g. Saving the network.\" % test_accuracy)\n",
    "        else:\n",
    "            saver.restore(sess, 'mnist_fc_best')\n",
    "            print(\"Validation accuracy was: %g. It was better before: %g. \" % (test_accuracy, best_accuracy) +\n",
    "                  \"Using the old params for further optimizations.\")\n",
    "    sess.run(train_step, feed_dict={x: input_images, y_: correct_predictions, keep_prob_input: 0.8, keep_prob: 0.5})\n",
    "print(\"The training took %.4f seconds.\" % (time() - start_time))\n",
    "\n",
    "# validate\n",
    "print(\"Best test accuracy: %g\" % best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
